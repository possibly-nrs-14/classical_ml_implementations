Machine Learning Algorithms Implementations

A collection of handsâ€‘on Jupyter notebooks that reâ€‘implement core machineâ€‘learning algorithms from scratch and walk through realâ€‘world datasets, complete with exploratory analysis, model training, evaluation and speed/accuracy tradeâ€‘off experiments.

ðŸ“‚ Repository Contents

Folder / Notebook

Algorithm / Topic

Highlights

knn.ipynb

Bruteâ€‘force kâ€‘Nearest Neighbours (PyTorch)

cosine / Euclidean distance kernels, batching on GPU

knn_ivf.ipynb

Inverted File (IVF) Approximateâ€‘KNN

coarse quantisation, probing lists,  10Ã— speedâ€‘up benchmarks

knn_lsh.ipynb

Localityâ€‘Sensitive Hashing (LSH)

random hyperâ€‘plane hashes, Hamming buckets, recall vs probes curves

decision_tree.ipynb

Decisionâ€‘Tree Classifier

entropy vs Gini, categorical encoding, fraudâ€‘detection case study

linear_regression.ipynb

Linear Regression

courierâ€‘delivery dataset, feature scaling, residual diagnostics

Why these notebooks?  They illustrate the spectrum from exact to approximate neighbour search, and from classic interpretable models to modern scalable DLâ€‘assisted pipelines.

Quick Start

Clone & install deps

git clone <repoâ€‘url>
cd mlâ€‘algorithmsâ€‘sandbox
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

Launch Jupyter

jupyter lab

Open any notebook and run all cells (most have CPUâ€‘only fallâ€‘backs, but GPUs are autoâ€‘detected when available).

Requirements

PythonÂ â‰¥Â 3.9

NumPy, Pandas, scikitâ€‘learn, Matplotlib, Seaborn

PyTorchÂ â‰¥Â 2.1 (optional but recommended for GPU acceleration)

tqdm, SciPy, ipywidgets (for interactive sliders)

Install everything with:

pip install -r requirements.txt

A readyâ€‘made requirements.txt is provided.

Data

Notebookâ€‘specific CSVs live in the /data folder or are autoâ€‘downloaded on first run.

Notebook

Dataset

Purpose

decision_tree.ipynb

train_data.csv, val_data.csv, test_data.csv

Transactionâ€‘fraud detection

linear_regression.ipynb

dataset.csv

Couriers & deliveryâ€‘time prediction

knn*

embeddings.npy (autoâ€‘generated demo)

Vector search benchmarks

Note: for privacy, raw proprietary data is not committed. The notebooks create synthetic or anonymised samples if the real files are absent.

Results

Approxâ€‘KNN: IVF achieves ~95â€¯% recall with 10Ã— speedâ€‘up over brute force on 100â€¯k Ã—128â€‘D vectors; LSH reaches 80â€¯% recall while fitting in RAM on laptops.

Decision Tree: AUCÂ â‰ˆÂ 0.89 on heldâ€‘out set; feature importance highlights highâ€‘value receivers as prime fraud signals.

Linear Regression: RÂ²Â â‰ˆÂ 0.78, MAEÂ â‰ˆÂ 2.3â€¯min; traffic & weather rank as top predictors.
